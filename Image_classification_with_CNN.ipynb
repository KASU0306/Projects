{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T16:56:43.366642Z",
     "iopub.status.busy": "2022-01-14T16:56:43.366170Z",
     "iopub.status.idle": "2022-01-14T16:56:43.371482Z",
     "shell.execute_reply": "2022-01-14T16:56:43.370282Z",
     "shell.execute_reply.started": "2022-01-14T16:56:43.366610Z"
    }
   },
   "source": [
    "## Lab Course: Distributed Data Analytics\n",
    "## Exercise Sheet 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiftyprcnt(train_dataset):\n",
    "    indices = torch.arange(int(0.5*len(train_dataset)))\n",
    "    train_50pcnt = data_utils.Subset(train_dataset, indices)\n",
    "    return train_50pcnt\n",
    "\n",
    "def dim_calc(width,kernel_size):\n",
    "    w,k,p,s=width,kernel_size,0,1\n",
    "    conv_op=(w-k+2*p)/s+1   \n",
    "    w=conv_op/2\n",
    "    conv_op=(w-k+2*p)/s+1  \n",
    "    conv_op=(conv_op-k+2*p)/s+1   \n",
    "    w=conv_op/2\n",
    "    return int(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis: Image Classification\n",
    "\n",
    "**Approach**:\n",
    "1) **Model creation**: Created the model \"base\" as per specifications gvien in the excercise. The output of the network is without applying softmax as the cross entropy loss funtion already contains it.otmax is applied while calulating accuracy. Kenel size is chosen as 3. THe number of input and output channels in convolution layers and number of neurons are chosen randomly keeping in mind of complexity of the model.  The number o neurons after flattening of the dimension of the feature caluted using a predefined funtion named \"dim_calc\". This function uses predefined formulas to calculate the width of the output features at each convolution layer and returns the width of the feature output of pool2 layer. \n",
    "\n",
    "2) To use only **50 percent of the training dataset**,\"data_utils.Subset(train_dataset, indices)\" is used with \"indices\" as a list of numbers in the range of (50 percent the total length of the actuual train data.\n",
    "\n",
    "3) Baseline image classification without any data augmentation or normalization is performed along with other configurations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base(nn.Module):\n",
    "    def __init__(self,in_ch,width,kernel):\n",
    "        super(base, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_channels=32, kernel_size=kernel)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel)\n",
    "        self.conv3 = nn.Conv2d(64,128,kernel)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(128*dim_calc(width,kernel)**2,100)\n",
    "        self.fc2 = nn.Linear(100,50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Normalization Effect (CNN)\n",
    "\n",
    "**Appraoch for data loading**\n",
    "\n",
    "1)**Data Augmentation**: the images are flipped using \"RandomHorizontalFlip\" & \"RandomVerticalFlip\",translated and scaled and translated using \"RandomAffine(degrees=0,translate=(0.1, 0.3)\". \n",
    "\n",
    "2)**Normalization**: Each channel of the image is normalized by substracting the mean (µ) of each feature and a division by the standard deviation (σ). Referring to \n",
    "https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c on 21st June,2022.\n",
    "\n",
    "3) All combinations of configurations are predefined for the training data using transforms.Compose(). For the test data, only normalization is added for the configuration \"with normalization\" and \" wiht augmentation and normalization\". For the configuration \"with baseline\" and \"with augmentatiions\", only basic transformations to tensor is used. To load these configurations when required, a function named \"data-laoder\" is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.programcreek.com/python/example/117699/torchvision.transforms.RandomAffine\n",
    "augmetnations = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),transforms.RandomVerticalFlip(0.2),\n",
    "                              transforms.RandomAffine(degrees=0,translate=(0.1, 0.3),scale=(1.1,1.2)),\n",
    "                              transforms.ToTensor()])\n",
    "\n",
    "augmentations_with_norm=transforms.Compose([\n",
    "                              transforms.RandomHorizontalFlip(p=0.5),\n",
    "                              transforms.RandomVerticalFlip(0.2),\n",
    "                              transforms.RandomAffine(degrees=0,translate=(0.1, 0.3),scale=(1.1,1.2)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],std=[0.247, 0.243, 0.261])])\n",
    "\n",
    "norms=transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                                             std=[0.247, 0.243, 0.261])])\n",
    "\n",
    "\n",
    "\n",
    "basic_transforms=transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_trfms_with_norm=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                                             std=[0.247, 0.243, 0.261])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_l=[\"baseline\",\"with augmetnations\",\"with noramlization\",\"augmentations_with_noramlization\"]\n",
    "def data_loader(conig):\n",
    "    if config==config_l[0]:\n",
    "        train_50pcnt = fiftyprcnt(datasets.CIFAR10(root='data', train=True,download=True, transform=basic_transforms))\n",
    "        test_dataset = datasets.CIFAR10(root='data', train=False,download=True, transform=basic_transforms)\n",
    "    elif config==config_l[1]:\n",
    "        train_50pcnt = fiftyprcnt(datasets.CIFAR10(root='data', train=True,download=True, transform=augmetnations))\n",
    "        test_dataset = datasets.CIFAR10(root='data', train=False,download=True, transform=basic_transforms)\n",
    "    elif config==config_l[2]:\n",
    "        train_50pcnt = fiftyprcnt(datasets.CIFAR10(root='data', train=True,download=True, transform=norms))\n",
    "        test_dataset = datasets.CIFAR10(root='data', train=False,download=True, transform=test_trfms_with_norm)\n",
    "    else:\n",
    "        train_50pcnt = fiftyprcnt(datasets.CIFAR10(root='data', train=True,download=True, transform=augmentations_with_norm))\n",
    "        test_dataset = datasets.CIFAR10(root='data', train=False,download=True, transform=test_trfms_with_norm)\n",
    "    return train_50pcnt,test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning with different configurations**. \n",
    "1) In learning, iterating through list of configrations **including baseline**, 50 percent train data and full test data are loaded and \"torch.utils.data.DataLoader\" is used to load the data in minibatches. \n",
    "\n",
    "2) **Batch size** is chosen as **128**. **Adam optimizer** with learning rate **0.001** is chosen. The \"cross entropy loss\" is used to back propogate on to update the weights. \n",
    "\n",
    "3) **Softmax** is applied on output of the model and is compared with actaul labels to get accuracies. Iterating through mininbatches of train and test data, tarin and test lossses  and accuracies are taken at each epoch and written to tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(config,optim,lr,kernel):\n",
    "    train_dataset,test_dataset=data_loader(config)\n",
    "    trainloader_CIF = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,num_workers = 2)\n",
    "    testloader_CIF = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "    width,in_ch=32,3\n",
    "    model = base(in_ch,width,kernel)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loss_epoch_l,test_loss_epoch_l=[],[]\n",
    "    writer = SummaryWriter(f\"conf/coniguration={config}\")\n",
    "    print(f\"for coniguration={config}\")\n",
    "    for j in range(40):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss_h,train_pred_l,test_pred_l,test_label,train_label=0,[],[],[],[]\n",
    "        for images, labels in trainloader_CIF:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat_train = model(images)\n",
    "            prob=F.softmax(y_hat_train, dim=1)\n",
    "            pred=[torch.argmax(j) for j in prob]\n",
    "            train_loss = criterion(y_hat_train, labels)\n",
    "            train_loss_h+=train_loss.item()*len(images)\n",
    "            train_pred_l=train_pred_l+pred\n",
    "            train_label=train_label+list(labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss_epoch=np.round((train_loss_h/len(train_dataset)),4)\n",
    "        train_loss_epoch_l.append(train_loss_epoch)\n",
    "        train_acc=np.round((np.array(train_pred_l)==np.array(train_label)).mean(),4)\n",
    "        # testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss_h=0\n",
    "            for images, labels in testloader_CIF:\n",
    "                y_hat_test = model(images)\n",
    "                prob=F.softmax(y_hat_test, dim=1)\n",
    "                pred=[torch.argmax(j) for j in prob]\n",
    "                test_loss = criterion(y_hat_test, labels)\n",
    "                test_loss_h+=test_loss.item()*len(images)\n",
    "                test_pred_l=test_pred_l+pred\n",
    "                test_label=test_label+list(labels)\n",
    "        test_acc=np.round((np.array(test_pred_l)==np.array(test_label))).mean()\n",
    "        test_loss_epoch=np.round((test_loss_h/len(test_dataset)),4)\n",
    "        test_loss_epoch_l.append(test_loss_epoch)\n",
    "        writer.add_scalar('Loss_CIFAR10/train', train_loss_epoch, j)\n",
    "        writer.add_scalar('Loss_CIFAR10/test', test_loss_epoch, j)\n",
    "        writer.add_scalar('Accuracy_CIFAR10/train', train_acc, j)\n",
    "        writer.add_scalar('Accuracy_CIFAR10/test', test_acc, j)\n",
    "        print(f\"Epoch {j} - train_loss : {train_loss_epoch},test loss : {test_loss_epoch},train_acc : {train_acc},test acc : {test_acc}\")\n",
    "    print(\"                                                                                 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim,kernel,lr=\"Adam\",3,0.001\n",
    "torch.manual_seed(4)\n",
    "for config in config_l:\n",
    "    learning(config,optim,lr,kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Network Regularization (CNN)\n",
    "\n",
    "**Approach for regularization**\n",
    "\n",
    "**Data**\n",
    "50 percent of data is taken with only baseline transformations i.e \"toTensor()\" as to have comparison with baseline.\n",
    "\n",
    "**Models**\n",
    "A new model named \"base_drop\" is created. The dropout is added to in fully connected network of the original model \"base\" with p=0.25. Remining network kept same. For L1 and L2 regularization, the original model without dropout i.e \"base\" is used. \n",
    "\n",
    "**Learning**\n",
    "For learning with different regularization techniques, a new function called \"learning_regu\" is created. This funtion checks for the name of regularization. If it is \"dropout\", it takes the model \"base_drop\". Inorder to handle droputs during testing, model.train() and model.eval() is used before testing and training. If it is l1 or l2, it takes original model \"base\". Then while calculating the losses, \n",
    "\n",
    "for **l1 regularization**, loss is calcualted as below,\n",
    "\n",
    "lamda=0.0001<br>\n",
    "l1_abs = sum(p.abs().sum() for p in model.parameters())<br>\n",
    "train_loss = train_loss + lamda * l1_abs<br>\n",
    "\n",
    "for **l2 regularization**, loss is calcualted as below \n",
    "\n",
    "lamda=0.001<br>\n",
    "l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())<br>\n",
    "train_loss = train_loss + lamda * l2_norm<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1207.0580.pdf\n",
    "# https://wandb.ai/authors/ayusht/reports/Implementing-Dropout-in-PyTorch-With-Example--VmlldzoxNTgwOTE\n",
    "class base_drop(nn.Module):\n",
    "    def __init__(self,in_ch,width,kernel):\n",
    "        super(base_drop, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_channels=32, kernel_size=kernel)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel)\n",
    "        self.conv3 = nn.Conv2d(64,128,kernel)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(128*dim_calc(width,kernel)**2,100)\n",
    "        self.fc2 = nn.Linear(100,50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.drop_fc=nn.Dropout(p=0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.drop_fc(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.drop_fc(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.drop_fc(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = fiftyprcnt(datasets.CIFAR10(root='data', train=True,download=True, transform=basic_transforms))\n",
    "test_dataset = datasets.CIFAR10(root='data', train=False,download=True, transform=basic_transforms)\n",
    "trainloader_CIF = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,num_workers = 2)\n",
    "testloader_CIF = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://androidkt.com/how-to-add-l1-l2-regularization-in-pytorch-loss-function/\n",
    "def learning_regu(reg,optim,lr,kernel,lamda):\n",
    "    width,in_ch=32,3\n",
    "    if reg==\"dropout\":\n",
    "        model = base_drop(in_ch,width,kernel)\n",
    "    else:\n",
    "        model = base(in_ch,width,kernel)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loss_epoch_l,test_loss_epoch_l=[],[]\n",
    "    writer = SummaryWriter(f\"conf/regularization={reg}\")\n",
    "    print(f\"for regularization={reg}\")\n",
    "    for j in range(40):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss_h,train_pred_l,test_pred_l,test_label,train_label=0,[],[],[],[]\n",
    "        for images, labels in trainloader_CIF:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat_train = model(images)\n",
    "            prob=F.softmax(y_hat_train, dim=1)\n",
    "            pred=[torch.argmax(j) for j in prob]\n",
    "            train_loss = criterion(y_hat_train, labels)\n",
    "            if reg==\"l2\":\n",
    "                lamda=0.001\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                train_loss = train_loss + lamda * l2_norm\n",
    "            elif reg==\"l1\":\n",
    "                lamda=0.0001\n",
    "                l1_abs = sum(p.abs().sum() for p in model.parameters())\n",
    "                train_loss = train_loss + lamda * l1_abs\n",
    "            train_loss_h+=train_loss.item()*len(images)\n",
    "            train_pred_l=train_pred_l+pred\n",
    "            train_label=train_label+list(labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss_epoch=np.round((train_loss_h/len(train_dataset)),4)\n",
    "        train_loss_epoch_l.append(train_loss_epoch)\n",
    "        train_acc=np.round((np.array(train_pred_l)==np.array(train_label)).mean(),4)\n",
    "        # testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss_h=0\n",
    "            for images, labels in testloader_CIF:\n",
    "                y_hat_test = model(images)\n",
    "                prob=F.softmax(y_hat_test, dim=1)\n",
    "                pred=[torch.argmax(j) for j in prob]\n",
    "                test_loss = criterion(y_hat_test, labels)\n",
    "                test_loss_h+=test_loss.item()*len(images)\n",
    "                test_pred_l=test_pred_l+pred\n",
    "                test_label=test_label+list(labels)\n",
    "        test_acc=np.round((np.array(test_pred_l)==np.array(test_label))).mean()\n",
    "        test_loss_epoch=np.round((test_loss_h/len(test_dataset)),4)\n",
    "        test_loss_epoch_l.append(test_loss_epoch)\n",
    "        writer.add_scalar('Loss_CIFAR10/train', train_loss_epoch, j)\n",
    "        writer.add_scalar('Loss_CIFAR10/test', test_loss_epoch, j)\n",
    "        writer.add_scalar('Accuracy_CIFAR10/train', train_acc, j)\n",
    "        writer.add_scalar('Accuracy_CIFAR10/test', test_acc, j)\n",
    "        print(f\"Epoch {j} - train_loss : {train_loss_epoch},test loss : {test_loss_epoch},train_acc : {train_acc},test acc : {test_acc}\")\n",
    "    print(\"                                                                                 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regu_l=[\"l2\",\"l1\",\"dropout\"]\n",
    "optim,kernel,lr,lamda=\"Adam\",3,0.001,0.001\n",
    "torch.manual_seed(4)\n",
    "for reg in regu_l:\n",
    "    learning_regu(reg,optim,lr,kernel,lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Optimizers (CNN)\n",
    "\n",
    "**Approach**\n",
    "**Data** 50 percent of data is taken with only baseline transformations i.e \"toTensor()\" as to have comparison with baseline. Here baseline optimizer is also Adam with learning rate 0.001. It is alos a part of following excercise.\n",
    "\n",
    "**Models**: the original model used for baseline i.e \"base\" is used.\n",
    "\n",
    "**Learning** For learning with different optimizers i.e SGD and Adam, a new function called \"learning_lr\" is created. This funtion takes the name of the optimizer as argument and learns with different learning rates i.e [0.01,0.001,0.00001]. Rest of the learning methodology is same as that of the baseline model. The respective train/test losses and accuracies are recorded in tensprboard for each combination of optimzer and learning rate. The same are analyzed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_sel(model,opt,lr):\n",
    "    if opt==\"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_lr(optim,kernel): \n",
    "    for lr in [0.01,0.001,0.00001]:\n",
    "        width,in_ch=32,3\n",
    "        model = base(in_ch,width,kernel)\n",
    "        optimizer = optim_sel(model,optim,lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loss_epoch_l,test_loss_epoch_l=[],[]\n",
    "        writer = SummaryWriter(f\"conf/optimizer={optim}_lr={lr}\")\n",
    "        print(f\"for optimizer={optim}_lr={lr}\")\n",
    "        for j in range(40):\n",
    "            # training\n",
    "            model.train()\n",
    "            train_loss_h,train_pred_l,test_pred_l,test_label,train_label=0,[],[],[],[]\n",
    "            for images, labels in trainloader_CIF:\n",
    "                optimizer.zero_grad()\n",
    "                y_hat_train = model(images)\n",
    "                prob=F.softmax(y_hat_train, dim=1)\n",
    "                pred=[torch.argmax(j) for j in prob]\n",
    "                train_loss = criterion(y_hat_train, labels)\n",
    "                train_loss_h+=train_loss.item()*len(images)\n",
    "                train_pred_l=train_pred_l+pred\n",
    "                train_label=train_label+list(labels)\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss_epoch=np.round((train_loss_h/len(train_dataset)),4)\n",
    "            train_loss_epoch_l.append(train_loss_epoch)\n",
    "            train_acc=np.round((np.array(train_pred_l)==np.array(train_label)).mean(),4)\n",
    "            # testing\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss_h=0\n",
    "                for images, labels in testloader_CIF:\n",
    "                    y_hat_test = model(images)\n",
    "                    prob=F.softmax(y_hat_test, dim=1)\n",
    "                    pred=[torch.argmax(j) for j in prob]\n",
    "                    test_loss = criterion(y_hat_test, labels)\n",
    "                    test_loss_h+=test_loss.item()*len(images)\n",
    "                    test_pred_l=test_pred_l+pred\n",
    "                    test_label=test_label+list(labels)\n",
    "            test_acc=np.round((np.array(test_pred_l)==np.array(test_label))).mean()\n",
    "            test_loss_epoch=np.round((test_loss_h/len(test_dataset)),4)\n",
    "            test_loss_epoch_l.append(test_loss_epoch)\n",
    "            writer.add_scalar('Loss_CIFAR10/train', train_loss_epoch, j)\n",
    "            writer.add_scalar('Loss_CIFAR10/test', test_loss_epoch, j)\n",
    "            writer.add_scalar('Accuracy_CIFAR10/train', train_acc, j)\n",
    "            writer.add_scalar('Accuracy_CIFAR10/test', test_acc, j)\n",
    "            print(f\"Epoch {j} - train_loss : {train_loss_epoch},test loss : {test_loss_epoch},train_acc : {train_acc},test acc : {test_acc}\")\n",
    "        print(\"                                                                                 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)\n",
    "for optim in [\"SGD\",\"Adam\"]:\n",
    "    learning_lr(optim,kernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
